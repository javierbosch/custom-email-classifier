students-bounces@inf.ed.ac.uk
---------------
[Students] Talks TODAY Lecturer/Senior Lecturer/Reader in Data	Centric Systems
---------------
The talks for Lecturer/Senior Lecturer/Reader in Data Centric Systems will take place TODAY. Please note all talks are confidential and should not be shared out with the school. 

 

We will be using Blackboard Collaborate for these talks, please use the following link when required: https://eu.bbcollab.com/guest/69d003c28be54b988c9b1ce8e57c2e51 

 

Due to the packed schedule there will unfortunately be no feedback sessions scheduled for these talks although feedback can be sent to the individuals listed below: 

Feedback on the Research talks can be sent by email to Jane Hillston (Jane.Hillston@ed.ac.uk <mailto:Jane.Hillston@ed.ac.uk> ) or Mahesh Marina (Mahesh@ed.ac.uk <mailto:Mahesh@ed.ac.uk> ) 

Feedback on the Teaching talks can be sent by email to Milos Nikolic (milos.nikolic@ed.ac.uk <mailto:milos.nikolic@ed.ac.uk> ) 

 

If anyone wishes to talk with the candidates please contact their hosts who will be arranging an online schedule for them. 

 

If anyone has any questions please just let me know and I will be more than happy to help. 

 

Best wishes, 

Sam 

 

------------

Monday 6th April

Research talks 

 

9.00am: Oana Balmau (host Antonio Barbalace antonio.barbalace@ed.ac.uk <mailto:antonio.barbalace@ed.ac.uk>  )

 

Biography: 

Oana Balmau is a PhD candidate in the School of Computer Science at the University of Sydney, advised by Prof. Willy Zwaenepoel. She earned her Bachelors and Masters degrees in Computer Science from EPFL, Switzerland. Her research interests are computer systems and storage technologies. Currently, she is working towards redesigning storage systems, aimed at future workloads and hardware. (Homepage: oanabalmau.com) 

 

Publications:

https://scholar.google.com/citations?user=RmDWGhAAAAAJ

 

Research talk: 

Redesigning Storage Systems for Future Workloads, Hardware, and Performance Requirements

 

Cloud storage stacks are being challenged by new workloads, new hardware and new performance requirements. First, workloads evolved from following a read-heavy pattern (e.g., a static web-page) to a write-heavy profile where the read:write ratio is closer to 1:1 (e.g., as in the Internet of Things). Second, the hardware is undergoing rapid changes.  The divide between fine-grained volatile memory and slow block-level storage is rapidly being bridged by the emerging byte-addressable non-volatile memory devices and the fast block-addressable NVMe SSDs (e.g., Intel Optane NVMe SSDs). Third, performance requirements in storage systems now emphasize low tail latency, in addition to high throughput. 

In this talk I will argue that existing storage systems have fundamental limitations that do not allow them to fully meet these challenges, and that therefore the storage stack needs to undergo radical change. In particular, using state-of-the-art key-value stores I will demonstrate that with modern workloads and hardware the bottleneck shifts from I/O to CPU, invalidating an assumption that has underpinned all past storage system design. In line with this observation I will then present a new design paradigm for key-value stores that departs from the conventional wisdom of optimizing disk usage and instead optimizes CPU usage. To do so, we keep data unsorted on disk, reduce contention for shared data structures, and do away with expensive maintenance operations.

This design has been implemented in the KVell key-value store. KVell outperforms state-of-the-art key-value stores such as RocksDB in both read- and write-heavy workloads, running on modern NVMe SSDs.  Thanks to its novel design, KVell achieves up to 5x better throughput, and up to two orders of magnitude lower tail latency.

 

Teaching talks run by Milos Nikolic 

The presentation should be in the form of a mock tutorial and be an introductory 15 minutes talk aimed at third year students on the MapReduce programming model and its application for large scale data analytics. The presentation will be attended by students (possibly of different levels) and staff, and there will be five minutes afterwards to ask questions about the presentation and about the candidates approach to teaching

 

9.45am: Oana Balmau

 

10.15am: Lao Mai

 

10.45am: Amir Shaikhha

 

Research talks 

 

11.15am: Break

 

11.20am: Lao Mai (host Hugh Leather hleather@inf.ed.ac.uk <mailto:hleather@inf.ed.ac.uk>  )

 

Biography: 

Dr Luo Mai is a research associate at Imperial College London. His research lies at the intersection between networked systems, Big data analytics and distributed machine learning. Luo obtained his PhD from Imperial College London in 2018. From 2015 to 2018, he was a visiting researcher at Microsoft. Luo is the recipient of Google Fellowship in Cloud Computing and Alibaba Innovative Research Award. He won the best open-source software award from ACM Multimedia 2017.

 

Publications:

https://www.doc.ic.ac.uk/~lm111/ 

 

Research talk: 

Designing Efficient Distributed Data-centric Systems

Data-centric systems are the core infrastructure of modern IT technology. To process big-data and large machine-learning models, these systems usually comprise a control plane to orchestrate computation and a data plane to process and transfer data. To achieve the highest possible efficiency when operating these systems, users need to carefully select the parameters for the control plane and update them whenever the workload changes. The data plane also needs to be designed such that network bottlenecks are eliminated and the system can exhibit linear scalability. In this talk, I will describe my recent work to (i) enable adaptive management of control-plane parameters in existing data-centric systems, and (ii) to realise application-specific in-network computing to address network bottlenecks in data centres. I will use a large-scale stream processing system that I designed with a leading big-data cloud provider as an example to demonstrate the opportunities and benefits of building an adaptive control plane and a scalable data plane.

 

12.05pm: Amir Shaikhha (host James Cheney jcheney@inf.ed.ac.uk <mailto:jcheney@inf.ed.ac.uk> ) 

 

Biography: 

Amir Shaikhha is a Departmental Lecturer in Computer Science at Oxford University. His research focuses on the design and implementation of data-analytics systems by using techniques from the programming languages, compilers, and databases communities. He earned his Ph.D. from EPFL in 2018, for which he was awarded a Google Ph.D. Fellowship in structured data analysis, as well as a Ph.D. thesis distinction award.

 

Publications:

https://scholar.google.com/citations?user=97wlbikAAAAJ 

 

Research talk: 

Compilation and Code Optimization for Data Analytics

The trade-offs between the use of modern high-level and low-level programming languages in constructing complex software artifacts are well known. High-level languages allow for greater programmer productivity: abstraction and genericity allow for the same functionality to be implemented with significantly less code compared to low-level languages. However, the use of high-level languages comes at a performance cost: increased indirection due to abstraction, virtualization, and interpretation, and superfluous work, particularly in the form of temporary memory allocation and deallocation to support objects and encapsulation. As a result of this, the cost of high-level languages for performance-critical systems may seem prohibitive. The vision of "abstraction without regret" argues that it is possible to use high-level languages for building performance-critical systems that allow for both productivity and high performance, instead of trading off the former for the latter. In this talk, we realize this vision for building different types of data analytics systems. Our means of achieving this is by employing compilation. The goal is to compile away expensive language features -- to compile high-level code down to efficient low-level code.

 

12.50pm: Hao Wang (host Paul Patras ppatras@inf.ed.ac.uk <mailto:ppatras@inf.ed.ac.uk> )

 

Biography: 

Hao Wang is a 5th year Ph.D. student at the University of Toronto under the supervision of Professor Baochun Li. Hao received both of his B.E. degree in Information Security and M.E. degree in Software Engineering from Shanghai Jiao Tong University in 2012 and 2015, respectively. His research interests include large-scale data analytics, distributed machine learning, and datacenter networking. He has published 16 papers (including eight first-author papers) in prestigious networking and system conferences and journals, such as INFOCOM, SoCC, TPDS, and ToN. For more information about Hao, please visit https://www.haow.ca/.

 

Publications:

https://www.haow.ca/

 

Research talk: 

Optimizing Distributed Machine Learning with Reinforcement Learning

In the era of Internet of Things, mobile computing, and Big Data, millions of sensors and mobile devices are constantly generating massive volumes of data. To utilize the vast amount of data without violating data privacy, Federated Learning has emerged as a new paradigm of distributed machine learning that orchestrates model training across mobile devices. In this talk, I will first introduce the current challenges in distributed machine learning, and then present my recent work on statistical heterogeneity in federated learning. Specifically, I will talk about applying reinforcement learning to optimize federated learning by learning the best choice of device selection. 

 

1.35pm: Break

 

1.40pm: Dingwen Tao (host Bjoern Franke bfranke@inf.ed.ac.uk <mailto:bfranke@inf.ed.ac.uk> )

 

Biography: 

Dr. Dingwen Tao is an Assistant Professor in the Department of Computer Science at the University of Alabama. He founded the High-Performance Data Analytics and Computing Lab at UA, and the lab has 3 Ph.D. students and 2 undergraduate students. He received his bachelor’s degree in mathematics from the University of Science and Technology of China in 2013 and his Ph.D. degree in computer science from the University of California, Riverside in 2018. Before joining the university as faculty, he interned at Pacific Northwest National Laboratory, Argonne National Laboratory, and Brookhaven National Laboratory. His research interests include high-performance computing, parallel and distributed systems, and big data analytics. Specifically, he focuses on scientific data reduction and management, resilience and fault tolerance, and large-scale machine learning & deep learning. He has published over 30 peer-reviewed high-quality papers in prestigious HPC and Big Data conferences and journals, such as ACM ICS, HPDC, PPoPP, SC, IEEE BigData, CLUSTER, IPDPS, MSST, TPDS, IJHPCA, including two Best Paper awards. His current research has been supported by U.S. NSF, DOE, and NOAA.

Publications:

https://www.dingwentao.com/publication

 

Research talk: 

Error-Bounded Lossy Compression Approach for Scientific Data Reduction in Exascale Computing

The next generation of supercomputers will be exascale high-performance computing (HPC) systems, which are capable of at least 1018 floating-point operations per second. These systems will help researchers tackle increasingly complex problems (such as nuclear reactors and global climate) through modeling and simulating large-scale systems. Due to the gap between the ever-increasing computation power and the limited storage capacity and I/O bandwidth, HPC researchers have to develop intelligent ways to efficiently manage the scientific data. Error-bounded lossy compression has been considered as a promising solution, because it can significantly reduce the data size while maintaining high data fidelity. However, the current HPC community is facing several challenges to effectively and efficiently utilize error-bounded lossy compression in practice. This talk will cover three related research topics, including: (1) developing and optimizing error-bounded lossy compression for extreme-scale HPC applications, (2) exploring error-bounded lossy compression in diverse computing scenarios, and (3) enhancing cyberinfrastructure via hardware-algorithm co-designed lossy compression. 

 

2.25pm: Jagadish Kotra (host Vijay Nagarajan vnagaraj@inf.ed.ac.uk <mailto:vnagaraj@inf.ed.ac.uk> )

 

Biography: 

Dr. Jagadish Kotra is a researcher at AMD Research (in Austin), where he is working on optimizing AMD’s Exascale Heterogeneous Processor (EHP) node architecture funded by U.S Department of Energy (DOE) for the past 2.5 years. He obtained his Ph.D from The Pennsylvania State University in USA in 2017. His research interests are in the areas of Computer Architecture, Operating Systems and Hardware-software co-design focusing on heterogeneous compute and memory systems. At Penn State, his Ph.D dissertation proposed solutions that targeted bridging the performance gap between the multi/many-core processors and memory. To that end, he proposed hardware-only, software-only and hardware-software co-design based solutions that spanned multiple layers of the system stack. Jagadish’s work appeared in several top-tier research venues like ASPLOS, MICRO, PLDI, FAST and SIGMETRICS. His MICRO-2016 co-authored paper was one of the best paper nominees. He has 11 patents that are either granted or under filing in US Patent Office (USPTO) from his stints at various industry labs including AMD and IBM as a full-time researcher and at Intel, VMware as an intern.

 

Publications:

https://jbk5155.github.io 

 

Research talk: 

A Cross-Stack Approach to Optimize High-End Computing Systems.

Heterogeneous systems are ubiquitous and are deployed in a wide variety of devices ranging from low-end mobile systems to high-end datacenters and supercomputers. This heterogeneity is prevalent in both compute (CPUs/GPUs) and memory (HBM/Off-chip DDR-x) resources. In this talk, I will present an overview of my on-going and prior research that targets optimizing such heterogeneous systems targeting high-end computing systems like datacenters and Supercomputers. Specifically, I will present a hardware-software co-designed heterogeneous memory system that reconfigures dynamically based on the workloads executing on the system. If time permits, I will briefly cover my other research projects that targets optimizing heterogeneous compute (CPU/GPU) systems before presenting future research directions.

 

3.10pm: Supreeth Shastri (host Pramod Bhatotia pramod.bhatotia@ed.ac.uk <mailto:pramod.bhatotia@ed.ac.uk> )

 

Biography: 

Supreeth Shastri is a Postdoctoral Fellow at the University of Texas at Austin hosted by Prof. Vijay Chidambaram. He received his PhD from the University of Massachusetts at Amherst in 2018 and MS from Columbia University in 2009. His research explores how computing systems are influenced by emerging trends in law and economics, and builds system infrastructure to manage their impact. His work has been adapted into best practices in the industry, deployed at the U.S. Federal Aviation Administration, integrated into courses at Brown and Penn State Universities, and received recognition from the NSF.

 

Publications:

https://www.cs.utexas.edu/~shastri/publications.html

 

Research talk: 

When Computing Meets Law and Economics

Design and operation of modern computer systems are increasingly influenced by external disciplines such as law and economics. In this talk, I highlight two such emerging phenomena: data protection laws and cloud economics. I make a case for why these trends are fundamental and how they invalidate computing principles and practices that have years of precedence. Then, I demonstrate how to build systems infrastructure to manage their impact. In particular, we will explore the General Data Protection Regulation (GDPR), the European law that has confounded even the technologically advanced companies. I will present my work on analyzing the law, building compliant storage systems, and measuring its impact. I will conclude by highlighting new directions for systems research in our interdisciplinary world.

 

 

Teaching talks run by Milos Nikolic 

The presentation should be in the form of a mock tutorial and be an introductory 15 minutes talk aimed at third year students on the MapReduce programming model and its application for large scale data analytics. The presentation will be attended by students (possibly of different levels) and staff, and there will be five minutes afterwards to ask questions about the presentation and about the candidates approach to teaching

 

3.55pm: Break

 

4.00pm: Hao Wang

 

4.30pm: Supreeth Shastri

 

5.00pm: Jagadish Kotra

 

5.30pm: Dingwen Tao 

 

 

------

Samantha Inch

Staffing Support Manager

HR Office

Room 5.39

University of Edinburgh, Informatics Forum

10 Crichton St | Edinburgh | EH8 9AB

Tel:  +44 131 650 9006

 

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.

 
